# Руководство по установке кластера ClickHouse
Практическое пошаговое руководство по развертыванию Вашего собственного кластера ClickHouse

# Архитектура
![clickhouse](https://github.com/user-attachments/assets/8d9c3a9c-0ba3-4f3f-afa5-c58df470bb59)

| Роль				| Это кто  			|Основная задача				| Порты				|
|-------------------|-------------------|-------------------------------|-------------------|
|ClickHouse Server	|Любая нода			|SQL, хранение, distributed		|9000 / 8123 / 9009	|
|Coordinator		|Любая нода			|План запроса					| —					|
|Shard				|Логическая сущность|Горизонтальное масштабирование	| —					|
|Replica			|Нода				|Отказоустойчивость				| —					|
|ZooKeeper			|Отдельный сервис	|Координация, метаданные		|2181 / 2888 / 3888	|
|Client				|Внешний			|Отправка запросов				|9000 / 8123		|

# Ссылки
1. Официальная документация\
Install ClickHouse on Debian/Ubuntu\
https://clickhouse.com/docs/install?ysclid=mkzsgud2sc374410174

2. Github\
Install_ClickHouse_Cluster\
https://gist.github.com/thanhthai3010/22f8d3d1f0d0897255b84f37571b683d

3. Официальная документация\
Движки таблиц\
https://clickhouse.com/docs/ru/engines/table-engines?ysclid=mldumst6yy94100096

4. ChatGPT



# Роли и компоненты

В ClickHouse важно сразу выкинуть «hadoop-мышление»: формальных master / worker ролей почти нет. Почти каждая нода — и сервер, и координатор.\
ClickHouse Server (часто называют «master-нода») - на самом деле все ClickHouse-ноды равноправны.

----------------------------------------------
1. ClickHouse Server — это универсальный узел, который одновременно может быть:
	* координатором запроса
	* хранителем данных
	* репликой
	* участником DDL ON CLUSTER
	* участником distributed insert/select


Нет выделенного master-а, любая нода может:\
	* принять запрос от клиента\
	* разослать подзапросы по кластеру\
	* агрегировать результат


Основные функции:\
	* Хранение данных (MergeTree, ReplicatedMergeTree)\
	* Выполнение SQL\
	* Межнодовое взаимодействие\
	* Репликация (через ZooKeeper)\
	* Distributed tables\
	* DDL ON CLUSTER

Порты ClickHouse Server

| Порт	| Назначение													|Протокол	|
|-------|---------------------------------------------------------------|-----------|
|9000	|Native client / server / distributed - основной «рабочий» порт	|TCP		|
|8123	|HTTP API - часто нужен BI / curl / REST						|HTTP		|
|9009	|Interserver (критичен для репликации)							|HTTP		|
|9440	|HTTPS (если включён)											|HTTPS		|


Роль определяется конфигурацией и запросом, а не типом ноды, пример:\
	* ты подключился к ch-master-1\
	* сделал SELECT FROM events_dist\
	* ch-master-1 стал координатором\
	* остальные — исполнителями

----------------------------------------------
2. Client (clickhouse-client, BI, приложение) - это источник запросов, а не часть кластера.
Может быть:
	* clickhouse-client
	* BI (Superset, Metabase, PowerBI)
	* backend-приложение
	* ETL job

Что делает:\
	* Подключается к любой ClickHouse-ноде\
	* Не знает о шардах\
	* Не знает о репликах\
	* Вся логика — внутри ClickHouse

Клиент никогда не общается с ZooKeeper\
Порты клиента:

|Порт|Когда используется|
|----|------------------|
|9000|clickhouse-client |
|8123|HTTP / BI			|
|9440|HTTPS				|

----------------------------------------------

2.3. ZooKeeper (координатор, а не data node) - в кластере нет мастер-ноды, как в GreenPlum / Hadoop, всю координацию выполняет ZooKeeper.\
ZooKeeper — это control plane ClickHouse, а не data plane.

Он НЕ:\
	* хранит данные таблиц\
	* участвует в запросах\
	* участвует в агрегациях

Он:\
	* хранит метаданные реплик\
	* выбирает лидера реплики\
	* хранит очередь репликации\
	* обеспечивает DDL ON CLUSTER

Что именно хранится в ZooKeeper:\
	* /clickhouse/tables/{shard}/{table}\
	* информация о репликах\
	* log репликации\
	* distributed DDL queue

Без ZooKeeper:\
	* репликации не существует\
	* ON CLUSTER не работает

Порты ZooKeeper
|Порт|Назначение			|
|----|----------------------|
|2181|Клиенты (ClickHouse)	|
|2888|Leader ↔ Follower		|
|3888|Leader election		|

Где он ставится:\
	* обычно нечетное кол-во нод: 3 или 5 нод\
	* может быть совмещён с ClickHouse (но это инфраструктурный компонент!)\
	* может быть отдельным кластером

----------------------------------------------

4. Логические роли внутри ClickHouse.
Это не ноды, а поведение в конкретный момент времени:
	* Координатор запроса - нода, куда пришёл запрос: планирует выполнение и агрегирует результат. Может быть любой нодой!
	* Шард - логическое разбиение данных, определяется в remote_servers.xml, один шард = набор реплик
	* Реплика - копия данных шарда, управляется через ZooKeeper, одна из реплик — leader
	* Leader реплики - выбирается ZooKeeper, принимает INSERT, остальные подтягивают данные

----------------------------------------------

5. Как это всё работает вместе.

INSERT в Distributed\
	* Клиент → любая CH-нода\
	* Эта нода = координатор\
	* Выбирается шард\
	* INSERT идёт в одну реплику\
	* Остальные реплики подтягивают данные через ZooKeeper


SELECT из Distributed\
	* Клиент → любая CH-нода\
	* Подзапросы → все шарды\
	* Реплика выбирается автоматически\
	* Координатор собирает результат

# Движки

ClickHouse-движки — это сердце философии CH. Тут не «одна таблица», а конструктор под задачу.\
ClickHouse — это:
	* append-only
	* immutable parts
	* async merge

Поэтому:
	* нет UPDATE / DELETE
	* всё решается на уровне движка
	* правильный выбор ENGINE = 50% успеха
	
Базовая семья: MergeTree (99% продовых таблиц)

	1. MergeTree - База всего

Что умеет\
	* Колончное хранение\
	* Партиции (PARTITION BY)\
	* Сортировка (ORDER BY) — ключевая вещь\
	* Индексы (primary key = sparse index)\
	* Асинхронные merge’ы\

Когда\
	* append-only факты\
	* логи\
	* события\
	* витрины BI\

ENGINE = MergeTree\
PARTITION BY toYYYYMM(dt)\
ORDER BY (user_id, dt)


	2. ReplacingMergeTree - “Псевдо-upsert”, при merge оставляем последнюю версию строки

Типовой кейс\
	* snapshots\
	* SCD1\
	* дедупликация

ENGINE = ReplacingMergeTree(version)

⚠️ Важно:\
	* физически старые версии есть, пока не смержились\
	* для точности нужен: SELECT ... FINAL

3. SummingMergeTree - Авто-агрегация

При merge:
	* строки с одинаковым ключом
	* числовые поля суммируются

Когда
	* pre-aggregated fact tables
	* счетчики
	* метрики

ENGINE = SummingMergeTree
ORDER BY (dt, product_id)

⚠️ Опасно для сложной логики — нет контроля над merge

4. AggregatingMergeTree - хардкор-агрегации

Хранит состояния агрегатных функций, а не числа:
	* sumState
	* uniqState
	* quantileState

Когда
	* OLAP-кубы
	* rollup-витрины
	* near-real-time BI

ENGINE = AggregatingMergeTree
ORDER BY (dt, key)

Чтение:

SELECT sumMerge(metric) FROM table


5. CollapsingMergeTree - строки-антистроки

Есть поле sign:
	* +1 — вставка
	* -1 — удаление

Когда
	* события изменений
	* CDC-подобные сценарии

⚠️ Сложный, редко нужен

6. VersionedCollapsingMergeTree - то же самое, что предыдущий, но:
	* с версией
	* решает конфликты порядка

Используется очень редко, но мощный.
